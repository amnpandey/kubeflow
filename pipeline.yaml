apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: fraud-detection-pipeline
  annotations:
    tekton.dev/output_artifacts: '{}'
    tekton.dev/input_artifacts: '{"download-dataset": [{"name": "create-pvc-name",
      "parent_task": "create-pvc"}], "export-model": [{"name": "create-pvc-name",
      "parent_task": "create-pvc"}], "train-model": [{"name": "create-pvc-name", "parent_task":
      "create-pvc"}]}'
    tekton.dev/artifact_bucket: mlpipeline
    tekton.dev/artifact_endpoint: minio-service.kubeflow:9000
    tekton.dev/artifact_endpoint_scheme: http://
    tekton.dev/artifact_items: '{"create-pvc": [], "download-dataset": [], "export-model":
      [], "train-model": []}'
    sidecar.istio.io/inject: "false"
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A sample pipeline to demonstrate
      multi-step model training, evaluation, export", "inputs": [{"default": "/train/data",
      "name": "data_dir", "optional": true, "type": "String"}, {"default": "/train/model",
      "name": "model_dir", "optional": true, "type": "String"}, {"default": "iaf-ai",
      "name": "export_bucket", "optional": true, "type": "String"}, {"default": "fraud-detection",
      "name": "model_name", "optional": true, "type": "String"}, {"default": "1",
      "name": "model_version", "optional": true, "type": "Integer"}, {"default": "/train/metrics",
      "name": "metrics_path", "optional": true, "type": "String"}], "name": "Fraud
      detection Pipeline"}'
spec:
  params:
  - {name: data_dir, value: /train/data}
  - {name: model_dir, value: /train/model}
  - {name: export_bucket, value: iaf-ai}
  - {name: model_name, value: fraud-detection}
  - {name: model_version, value: '1'}
  - {name: metrics_path, value: /train/metrics}
  pipelineSpec:
    params:
    - {name: data_dir, default: /train/data}
    - {name: model_dir, default: /train/model}
    - {name: export_bucket, default: iaf-ai}
    - {name: model_name, default: fraud-detection}
    - {name: model_version, default: '1'}
    - {name: metrics_path, default: /train/metrics}
    tasks:
    - name: create-pvc
      params:
      - {name: action, value: create}
      - name: output
        value: |
          - name: manifest
            valueFrom: '{}'
          - name: name
            valueFrom: '{.metadata.name}'
          - name: size
            valueFrom: '{.status.capacity.storage}'
      - name: set-ownerreference
        value: "false"
      taskSpec:
        params:
        - {description: Action on the resource, name: action, type: string}
        - {default: strategic, description: Merge strategy when using action patch,
          name: merge-strategy, type: string}
        - {default: '', description: An express to retrieval data from resource.,
          name: output, type: string}
        - {default: '', description: A label selector express to decide if the action
            on resource is success., name: success-condition, type: string}
        - {default: '', description: A label selector express to decide if the action
            on resource is failure., name: failure-condition, type: string}
        - {default: 'index.docker.io/aipipeline/kubeclient:v0.0.2', description: Kubectl
            wrapper image, name: image, type: string}
        - default: "false"
          description: Enable set owner reference for created resource.
          name: set-ownerreference
          type: string
        steps:
        - args:
          - --action=$(params.action)
          - --merge-strategy=$(params.merge-strategy)
          - |
            --manifest=apiVersion: v1
            kind: PersistentVolumeClaim
            metadata:
              name: $(PIPELINERUN)-fraud-detection-pvc
            spec:
              accessModes:
              - ReadWriteMany
              resources:
                requests:
                  storage: 10Gi
              storageClassName: csi-cephfs
          - --output=$(params.output)
          - --success-condition=$(params.success-condition)
          - --failure-condition=$(params.failure-condition)
          - --set-ownerreference=$(params.set-ownerreference)
          image: $(params.image)
          name: main
          resources: {}
          env:
          - name: PIPELINERUN
            valueFrom:
              fieldRef: {fieldPath: 'metadata.labels[''tekton.dev/pipelineRun'']'}
        results:
        - {name: manifest, description: '{}'}
        - {name: name, description: '{.metadata.name}'}
        - {name: size, description: '{.status.capacity.storage}'}
      timeout: 0s
    - name: download-dataset
      params:
      - {name: create-pvc-name, value: $(tasks.create-pvc.results.name)}
      - {name: data_dir, value: $(params.data_dir)}
      taskSpec:
        steps:
        - name: main
          args: [--data-dir, $(inputs.params.data_dir)]
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def download_dataset(data_dir):\n    \"\"\"Download the Fraud Detection\
            \ data set to the KFP volume to share it among all steps\"\"\"\n    import\
            \ subprocess\n    import sys\n    import time\n\n    subprocess.check_call([sys.executable,\
            \ \"-m\", \"pip\", \"install\", \"minio\"])\n    time.sleep(5)\n\n   \
            \ import os\n    from minio import Minio\n    url=\"minio-kubeflow.apps.sun.cp.fyre.ibm.com\"\
            \n    key='minio'\n    secret='minio123'\n\n    if not os.path.exists(data_dir):\n\
            \        os.makedirs(data_dir)\n    print(\"Directory created successfully\"\
            )    \n\n    client = Minio(url, key, secret, secure=False)\n    client.fget_object('iaf-ai',\
            \ 'datasets/fraud-detections/dataset.csv', data_dir+'/dataset.csv')\n\
            \    print(\"Dataset downloaded successfully.\")\n    print(os.listdir(data_dir))\n\
            \nimport argparse\n_parser = argparse.ArgumentParser(prog='Download dataset',\
            \ description='Download the Fraud Detection data set to the KFP volume\
            \ to share it among all steps')\n_parser.add_argument(\"--data-dir\",\
            \ dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parsed_args = vars(_parser.parse_args())\n\n_outputs = download_dataset(**_parsed_args)\n"
          image: mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0
          volumeMounts:
          - {mountPath: /train, name: create-pvc}
        params:
        - {name: create-pvc-name}
        - {name: data_dir}
        volumes:
        - name: create-pvc
          persistentVolumeClaim: {claimName: $(inputs.params.create-pvc-name)}
        metadata:
          annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Download
              the Fraud Detection data set to the KFP volume to share it among all
              steps", "implementation": {"container": {"args": ["--data-dir", {"inputValue":
              "data_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
              \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "def download_dataset(data_dir):\n    \"\"\"Download the Fraud Detection
              data set to the KFP volume to share it among all steps\"\"\"\n    import
              subprocess\n    import sys\n    import time\n\n    subprocess.check_call([sys.executable,
              \"-m\", \"pip\", \"install\", \"minio\"])\n    time.sleep(5)\n\n    import
              os\n    from minio import Minio\n    url=\"minio-kubeflow.apps.sun.cp.fyre.ibm.com\"\n    key=''minio''\n    secret=''minio123''\n\n    if
              not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n    print(\"Directory
              created successfully\")    \n\n    client = Minio(url, key, secret,
              secure=False)\n    client.fget_object(''iaf-ai'', ''datasets/fraud-detections/dataset.csv'',
              data_dir+''/dataset.csv'')\n    print(\"Dataset downloaded successfully.\")\n    print(os.listdir(data_dir))\n\nimport
              argparse\n_parser = argparse.ArgumentParser(prog=''Download dataset'',
              description=''Download the Fraud Detection data set to the KFP volume
              to share it among all steps'')\n_parser.add_argument(\"--data-dir\",
              dest=\"data_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
              = vars(_parser.parse_args())\n\n_outputs = download_dataset(**_parsed_args)\n"],
              "image": "mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0"}}, "inputs":
              [{"name": "data_dir", "type": "String"}], "name": "Download dataset"}'}
      timeout: 0s
    - name: train-model
      params:
      - {name: create-pvc-name, value: $(tasks.create-pvc.results.name)}
      - {name: data_dir, value: $(params.data_dir)}
      - {name: model_dir, value: $(params.model_dir)}
      taskSpec:
        steps:
        - name: main
          args: [--data-dir, $(inputs.params.data_dir), --model-dir, $(inputs.params.model_dir)]
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - "def train_model(data_dir, model_dir):\n    \"\"\"Trains a CNN for 50\
            \ epochs using a pre-downloaded dataset.\n    Once trained, the model\
            \ is persisted to `model_dir`.\"\"\"\n\n    import tensorflow as tf\n\
            \    from tensorflow import keras\n    from tensorflow.keras import Sequential\n\
            \    from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n\
            \    from tensorflow.keras.layers import Conv1D, MaxPool1D\n    from tensorflow.keras.optimizers\
            \ import Adam\n    from tensorflow.keras.models import load_model\n\n\
            \    import pandas as pd\n    import numpy as np\n    from pathlib import\
            \ Path\n    from numpy import asarray\n    from numpy import argmax\n\
            \    import matplotlib.pyplot as plt\n    from sklearn.model_selection\
            \ import train_test_split\n    from sklearn.preprocessing import StandardScaler\n\
            \n    print(tf.__version__)\n\n    data = pd.read_csv(data_dir+\"/dataset.csv\"\
            )\n\n    print(data.head())\n    print(data.shape)\n\n    non_fraud =\
            \ data[data['Class']==0]\n    fraud = data[data['Class']==1]\n    non_fraud\
            \ = non_fraud.sample(fraud.shape[0])\n    data = fraud.append(non_fraud,\
            \ ignore_index=True)\n\n    print(\"Class data value count after balancing\
            \ the dataset\")\n    print(data['Class'].value_counts())\n\n    X = data.drop('Class',\
            \ axis = 1)\n    y = data['Class']\n\n    X_train, X_test, y_train, y_test\
            \ = train_test_split(X, y, test_size = 0.2, random_state = 0, stratify\
            \ = y)\n\n    print('Training set size: ', len(X_train))\n    print(X_train.head(),\
            \ y_train.head())\n\n    print('Validation set size: ', len(X_test))\n\
            \    print(X_test.head(), y_test.head())\n\n    scaler = StandardScaler()\n\
            \    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n\
            \n    y_train = y_train.to_numpy()\n    y_test = y_test.to_numpy()\n\n\
            \    # CNN model need 3d array, so need to reshape it\n    X_train = X_train.reshape(X_train.shape[0],\
            \ X_train.shape[1], 1)\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],\
            \ 1)\n\n    print(\"After reshapping the train and test datasets to 3D\
            \ array\")\n    print(X_train.shape, X_test.shape)\n\n    print(\"Building\
            \ the model\")\n    epochs = 50\n    model = Sequential()\n    model.add(Conv1D(32,\
            \ 2, activation='relu', input_shape = X_train[0].shape))\n    model.add(BatchNormalization())\n\
            \    model.add(MaxPool1D(2))\n    model.add(Dropout(0.2))\n\n    model.add(Conv1D(64,\
            \ 2, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPool1D(2))\n\
            \    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n    model.add(Dense(64,\
            \ activation='relu'))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1,\
            \ activation='sigmoid'))\n\n    model.compile(optimizer=Adam(lr=0.0001),\
            \ loss = 'binary_crossentropy', metrics=['accuracy'])\n    history = model.fit(X_train,\
            \ y_train, epochs=epochs, validation_data=(X_test, y_test), verbose=0)\n\
            \n    print(\"Model Summary: \")\n    model.summary()\n\n    # evaluate\
            \ the model\n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n\
            \    print('Accuracy: %.3f' % acc)\n\n    # Create directories if not\
            \ exists\n    Path(model_dir).mkdir(parents=True, exist_ok=True)\n   \
            \ print(\"Saving the model\")\n    model.save(model_dir+\"/\"+\"model.h5\"\
            ) \n\n    print(\"Model save successfully.\")\n\n    # evaluate the model\n\
            \    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print('Accuracy:\
            \ %.3f' % acc)\n    metrics = {\n        \"metrics\": [\n            {\"\
            name\": \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"\
            },\n            {\"name\": \"accuracy\", \"numberValue\": str(acc), \"\
            format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    #with open(metrics_path+\"\
            /mlpipeline_metrics.json\", \"w\") as f:\n    #    json.dump(metrics,\
            \ f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train\
            \ model', description='Trains a CNN for 50 epochs using a pre-downloaded\
            \ dataset.')\n_parser.add_argument(\"--data-dir\", dest=\"data_dir\",\
            \ type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
            --model-dir\", dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n\
            _parsed_args = vars(_parser.parse_args())\n\n_outputs = train_model(**_parsed_args)\n"
          image: mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0
          volumeMounts:
          - {mountPath: /train, name: create-pvc}
        params:
        - {name: create-pvc-name}
        - {name: data_dir}
        - {name: model_dir}
        volumes:
        - name: create-pvc
          persistentVolumeClaim: {claimName: $(inputs.params.create-pvc-name)}
        metadata:
          annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Trains
              a CNN for 50 epochs using a pre-downloaded dataset.", "implementation":
              {"container": {"args": ["--data-dir", {"inputValue": "data_dir"}, "--model-dir",
              {"inputValue": "model_dir"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
              \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
              "def train_model(data_dir, model_dir):\n    \"\"\"Trains a CNN for 50
              epochs using a pre-downloaded dataset.\n    Once trained, the model
              is persisted to `model_dir`.\"\"\"\n\n    import tensorflow as tf\n    from
              tensorflow import keras\n    from tensorflow.keras import Sequential\n    from
              tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization\n    from
              tensorflow.keras.layers import Conv1D, MaxPool1D\n    from tensorflow.keras.optimizers
              import Adam\n    from tensorflow.keras.models import load_model\n\n    import
              pandas as pd\n    import numpy as np\n    from pathlib import Path\n    from
              numpy import asarray\n    from numpy import argmax\n    import matplotlib.pyplot
              as plt\n    from sklearn.model_selection import train_test_split\n    from
              sklearn.preprocessing import StandardScaler\n\n    print(tf.__version__)\n\n    data
              = pd.read_csv(data_dir+\"/dataset.csv\")\n\n    print(data.head())\n    print(data.shape)\n\n    non_fraud
              = data[data[''Class'']==0]\n    fraud = data[data[''Class'']==1]\n    non_fraud
              = non_fraud.sample(fraud.shape[0])\n    data = fraud.append(non_fraud,
              ignore_index=True)\n\n    print(\"Class data value count after balancing
              the dataset\")\n    print(data[''Class''].value_counts())\n\n    X =
              data.drop(''Class'', axis = 1)\n    y = data[''Class'']\n\n    X_train,
              X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state
              = 0, stratify = y)\n\n    print(''Training set size: '', len(X_train))\n    print(X_train.head(),
              y_train.head())\n\n    print(''Validation set size: '', len(X_test))\n    print(X_test.head(),
              y_test.head())\n\n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test
              = scaler.transform(X_test)\n\n    y_train = y_train.to_numpy()\n    y_test
              = y_test.to_numpy()\n\n    # CNN model need 3d array, so need to reshape
              it\n    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],
              1)\n    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n\n    print(\"After
              reshapping the train and test datasets to 3D array\")\n    print(X_train.shape,
              X_test.shape)\n\n    print(\"Building the model\")\n    epochs = 50\n    model
              = Sequential()\n    model.add(Conv1D(32, 2, activation=''relu'', input_shape
              = X_train[0].shape))\n    model.add(BatchNormalization())\n    model.add(MaxPool1D(2))\n    model.add(Dropout(0.2))\n\n    model.add(Conv1D(64,
              2, activation=''relu''))\n    model.add(BatchNormalization())\n    model.add(MaxPool1D(2))\n    model.add(Dropout(0.5))\n\n    model.add(Flatten())\n    model.add(Dense(64,
              activation=''relu''))\n    model.add(Dropout(0.5))\n\n    model.add(Dense(1,
              activation=''sigmoid''))\n\n    model.compile(optimizer=Adam(lr=0.0001),
              loss = ''binary_crossentropy'', metrics=[''accuracy''])\n    history
              = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_test,
              y_test), verbose=0)\n\n    print(\"Model Summary: \")\n    model.summary()\n\n    #
              evaluate the model\n    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n    print(''Accuracy:
              %.3f'' % acc)\n\n    # Create directories if not exists\n    Path(model_dir).mkdir(parents=True,
              exist_ok=True)\n    print(\"Saving the model\")\n    model.save(model_dir+\"/\"+\"model.h5\")
              \n\n    print(\"Model save successfully.\")\n\n    # evaluate the model\n    loss,
              acc = model.evaluate(X_test, y_test, verbose=0)\n    print(''Accuracy:
              %.3f'' % acc)\n    metrics = {\n        \"metrics\": [\n            {\"name\":
              \"loss\", \"numberValue\": str(loss), \"format\": \"PERCENTAGE\"},\n            {\"name\":
              \"accuracy\", \"numberValue\": str(acc), \"format\": \"PERCENTAGE\"},\n        ]\n    }\n\n    #with
              open(metrics_path+\"/mlpipeline_metrics.json\", \"w\") as f:\n    #    json.dump(metrics,
              f)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
              model'', description=''Trains a CNN for 50 epochs using a pre-downloaded
              dataset.'')\n_parser.add_argument(\"--data-dir\", dest=\"data_dir\",
              type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-dir\",
              dest=\"model_dir\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
              = vars(_parser.parse_args())\n\n_outputs = train_model(**_parsed_args)\n"],
              "image": "mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0"}}, "inputs":
              [{"name": "data_dir", "type": "String"}, {"name": "model_dir", "type":
              "String"}], "name": "Train model"}'}
      runAfter: [download-dataset]
      timeout: 0s
    - name: export-model
      params:
      - {name: create-pvc-name, value: $(tasks.create-pvc.results.name)}
      - {name: export_bucket, value: $(params.export_bucket)}
      - {name: model_dir, value: $(params.model_dir)}
      - {name: model_name, value: $(params.model_name)}
      - {name: model_version, value: $(params.model_version)}
      taskSpec:
        steps:
        - name: main
          args: [--model-dir, $(inputs.params.model_dir), --export-bucket, $(inputs.params.export_bucket),
            --model-name, $(inputs.params.model_name), --model-version, $(inputs.params.model_version)]
          command:
          - sh
          - -ec
          - |
            program_path=$(mktemp)
            printf "%s" "$0" > "$program_path"
            python3 -u "$program_path" "$@"
          - |
            def export_model(
                model_dir,
                export_bucket,
                model_name,
                model_version,
            ):
                import os
                import boto3
                from botocore.client import Config

                s3 = boto3.client(
                    "s3",
                    endpoint_url="http://minio-kubeflow.apps.sun.cp.fyre.ibm.com",
                    aws_access_key_id="minio",
                    aws_secret_access_key="minio123",
                    config=Config(signature_version="s3v4"),
                )

                # Create export bucket if it does not yet exist
                response = s3.list_buckets()
                export_bucket_exists = False

                for bucket in response["Buckets"]:
                    if bucket["Name"] == export_bucket:
                        export_bucket_exists = True

                if not export_bucket_exists:
                    s3.create_bucket(ACL="public-read-write", Bucket=export_bucket)

                # Save model files to S3
                for root, dirs, files in os.walk(model_dir):
                    for filename in files:
                        local_path = os.path.join(root, filename)
                        s3_path = os.path.relpath(local_path, model_dir)

                        s3.upload_file(
                            local_path,
                            export_bucket,
                            f"models/{model_name}/{model_version}/{s3_path}",
                            ExtraArgs={"ACL": "public-read"},
                        )

                response = s3.list_objects(Bucket=export_bucket)
                print(f"All objects in {export_bucket}:")
                for file in response["Contents"]:
                    print("{}/{}".format(export_bucket, file["Key"]))

            import argparse
            _parser = argparse.ArgumentParser(prog='Export model', description='')
            _parser.add_argument("--model-dir", dest="model_dir", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--export-bucket", dest="export_bucket", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-name", dest="model_name", type=str, required=True, default=argparse.SUPPRESS)
            _parser.add_argument("--model-version", dest="model_version", type=int, required=True, default=argparse.SUPPRESS)
            _parsed_args = vars(_parser.parse_args())

            _outputs = export_model(**_parsed_args)
          image: mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0
          volumeMounts:
          - {mountPath: /train, name: create-pvc}
        params:
        - {name: create-pvc-name}
        - {name: export_bucket}
        - {name: model_dir}
        - {name: model_name}
        - {name: model_version}
        volumes:
        - name: create-pvc
          persistentVolumeClaim: {claimName: $(inputs.params.create-pvc-name)}
        metadata:
          annotations: {pipelines.kubeflow.org/component_spec: '{"implementation":
              {"container": {"args": ["--model-dir", {"inputValue": "model_dir"},
              "--export-bucket", {"inputValue": "export_bucket"}, "--model-name",
              {"inputValue": "model_name"}, "--model-version", {"inputValue": "model_version"}],
              "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
              > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def export_model(\n    model_dir,\n    export_bucket,\n    model_name,\n    model_version,\n):\n    import
              os\n    import boto3\n    from botocore.client import Config\n\n    s3
              = boto3.client(\n        \"s3\",\n        endpoint_url=\"http://minio-kubeflow.apps.sun.cp.fyre.ibm.com\",\n        aws_access_key_id=\"minio\",\n        aws_secret_access_key=\"minio123\",\n        config=Config(signature_version=\"s3v4\"),\n    )\n\n    #
              Create export bucket if it does not yet exist\n    response = s3.list_buckets()\n    export_bucket_exists
              = False\n\n    for bucket in response[\"Buckets\"]:\n        if bucket[\"Name\"]
              == export_bucket:\n            export_bucket_exists = True\n\n    if
              not export_bucket_exists:\n        s3.create_bucket(ACL=\"public-read-write\",
              Bucket=export_bucket)\n\n    # Save model files to S3\n    for root,
              dirs, files in os.walk(model_dir):\n        for filename in files:\n            local_path
              = os.path.join(root, filename)\n            s3_path = os.path.relpath(local_path,
              model_dir)\n\n            s3.upload_file(\n                local_path,\n                export_bucket,\n                f\"models/{model_name}/{model_version}/{s3_path}\",\n                ExtraArgs={\"ACL\":
              \"public-read\"},\n            )\n\n    response = s3.list_objects(Bucket=export_bucket)\n    print(f\"All
              objects in {export_bucket}:\")\n    for file in response[\"Contents\"]:\n        print(\"{}/{}\".format(export_bucket,
              file[\"Key\"]))\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Export
              model'', description='''')\n_parser.add_argument(\"--model-dir\", dest=\"model_dir\",
              type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--export-bucket\",
              dest=\"export_bucket\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-name\",
              dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--model-version\",
              dest=\"model_version\", type=int, required=True, default=argparse.SUPPRESS)\n_parsed_args
              = vars(_parser.parse_args())\n\n_outputs = export_model(**_parsed_args)\n"],
              "image": "mesosphere/kubeflow:1.0.1-0.5.0-tensorflow-2.2.0"}}, "inputs":
              [{"name": "model_dir", "type": "String"}, {"name": "export_bucket",
              "type": "String"}, {"name": "model_name", "type": "String"}, {"name":
              "model_version", "type": "Integer"}], "name": "Export model"}'}
      runAfter: [train-model]
      timeout: 0s
  timeout: 0s
